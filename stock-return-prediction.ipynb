{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Applying ML models for stock returns prediction**"
      ],
      "metadata": {
        "id": "yt-azE-T2REv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UagZDhJdZnSG"
      },
      "source": [
        "**Import required library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DKmMvQ2H-GXi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "import statsmodels.api as sm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRnTxbRXcV0r"
      },
      "source": [
        "**Data Collection and Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bd5K6i86-Ths"
      },
      "outputs": [],
      "source": [
        "# Define tickers\n",
        "tickers = ['AAPL', 'AMZN', 'JPM', 'PG', 'UNH']\n",
        "market_index = '^GSPC'  # S&P 500\n",
        "# Define time period (5 years)\n",
        "period = '5y'\n",
        "# Function to fetch data and calculate features\n",
        "def fetch_and_process_data(ticker, market_index, period):\n",
        "    # Fetch stock data\n",
        "    stock_data = yf.download(ticker, period=period)\n",
        "    market_data = yf.download(market_index, period=period)\n",
        "    # Ensure both datasets have the same dates\n",
        "    stock_data = stock_data.loc[stock_data.index.isin(market_data.index)]\n",
        "    market_data = market_data.loc[market_data.index.isin(stock_data.index)]\n",
        "    # Calculate stock returns (target variable)\n",
        "    stock_data['Returns'] = stock_data['Close'].pct_change().shift(-1)\n",
        "    # Feature Engineering\n",
        "    # Compute technical indicators\n",
        "    stock_data['SMA_10'] = stock_data['Close'].rolling(window=10, min_periods=1).mean()\n",
        "    stock_data['SMA_50'] = stock_data['Close'].rolling(window=50, min_periods=1).mean()\n",
        "    # Relative Strength Index (RSI)\n",
        "    delta = stock_data['Close'].diff()\n",
        "    gain = delta.where(delta > 0, 0).rolling(window=14, min_periods=1).mean()\n",
        "    loss = -delta.where(delta < 0, 0).rolling(window=14, min_periods=1).mean()\n",
        "    rs = gain / loss\n",
        "    stock_data['RSI_14'] = 100 - (100 / (1 + rs))\n",
        "    # Moving Average Convergence Divergence (MACD)\n",
        "    stock_data['MACD'] = stock_data['Close'].ewm(span=12, adjust=False).mean() - stock_data['Close'].ewm(span=26, adjust=False).mean()\n",
        "    # Bollinger Bands\n",
        "    stock_data['Rolling_std'] = stock_data['Close'].rolling(window=10).std()\n",
        "    stock_data['BB_Upper'] = stock_data['SMA_10'] + 2 * stock_data['Rolling_std']\n",
        "    stock_data['BB_Lower'] = stock_data['SMA_10'] - 2 * stock_data['Rolling_std']\n",
        "    # Average True Range (ATR)\n",
        "    high_low = stock_data['High'] - stock_data['Low']\n",
        "    high_close = (stock_data['High'] - stock_data['Close']).abs()\n",
        "    low_close = (stock_data['Low'] - stock_data['Close']).abs()\n",
        "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "    stock_data['ATR'] = true_range.rolling(window=14).mean()\n",
        "    # Market indicators\n",
        "    market_data['MarketReturns'] = market_data['Close'].pct_change()\n",
        "    market_features = market_data[['MarketReturns']]\n",
        "    # Add market returns to stock data\n",
        "    combined_data = pd.merge(stock_data, market_features, left_index=True, right_index=True)\n",
        "    # Drop NaN values\n",
        "    combined_data = combined_data.dropna()\n",
        "\n",
        "    return combined_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eek_NH5F0Tnr"
      },
      "outputs": [],
      "source": [
        "#check for null or na\n",
        "for ticker in tickers:\n",
        "    print(f\"Processing {ticker}...\")\n",
        "    data = fetch_and_process_data(ticker, market_index, period)\n",
        "    print(data.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define evaluation metrics ,features and target**"
      ],
      "metadata": {
        "id": "rkN4ESRT5HMj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ElXKgbGDlqA4"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "models = {}\n",
        "predictions = {}\n",
        "importances = {}\n",
        "eval_metrics = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'MAE', 'R2'])\n",
        "cv_results = {}\n",
        "# Define features and target\n",
        "target_col = 'Returns'\n",
        "feature_cols = ['SMA_10', 'SMA_50', 'RSI_14', 'MACD', 'BB_Upper', 'BB_Lower', 'ATR','MarketReturns']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T18xYJq-dlqn"
      },
      "source": [
        "**Model Implementation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GeZ1HS6dyim"
      },
      "source": [
        "* Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S9SYToLt9zeE"
      },
      "outputs": [],
      "source": [
        "# Process data for each ticker\n",
        "for ticker in tickers:\n",
        "    print(f\"Processing {ticker}...\")\n",
        "    #Data Preprocessing\n",
        "    data = fetch_and_process_data(ticker, market_index, period)\n",
        "    X = data[feature_cols]\n",
        "    y = data[target_col]\n",
        "    # Split data (keeping time order) - 80% training, 20% testing\n",
        "    train_size = int(len(X) * 0.8)\n",
        "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    # Model Implementation\n",
        "    # 1. Linear Regression\n",
        "    print(\"Training Linear Regression...\")\n",
        "    lr_model = LinearRegression()\n",
        "    lr_model.fit(X_train_scaled, y_train)\n",
        "    lr_pred = lr_model.predict(X_test_scaled)\n",
        "    models['Linear Regression'] = lr_model\n",
        "    predictions['Linear Regression'] = lr_pred\n",
        "    # Add constant for intercept\n",
        "    X_train_with_const = sm.add_constant(X_train_scaled)\n",
        "    # Fit OLS model\n",
        "    ols_model = sm.OLS(y_train, X_train_with_const).fit()\n",
        "    # Get p-values\n",
        "    p_values = ols_model.pvalues[1:]  # Skip intercept p-value\n",
        "    # Create importance DataFrame with both coefficients and p-values\n",
        "    importances['Linear Regression'] = pd.DataFrame({\n",
        "      'Feature': feature_cols,\n",
        "      'Importance': np.abs(lr_model.coef_),\n",
        "      'Coefficient': ols_model.params[1:],  # Skip intercept\n",
        "      'Abs_Coefficient': np.abs(ols_model.params[1:]),\n",
        "      'p_value': p_values,\n",
        "      'Significant': p_values < 0.05  # Flag for significant features\n",
        "    }).sort_values('Abs_Coefficient', ascending=False)\n",
        "    print(importances['Linear Regression'].sort_values('Feature', ascending=True)[['Feature','p_value', 'Significant']])\n",
        "    # Model Evaluation\n",
        "    # Create evaluation metrics dataframe\n",
        "    for model_name, y_pred in predictions.items():\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        eval_metrics = eval_metrics._append({\n",
        "            'Model': model_name,\n",
        "            'MSE': mse,\n",
        "            'MAE': mae,\n",
        "            'R2': r2\n",
        "        }, ignore_index=True)\n",
        "    # Print metrics\n",
        "    print(f\"Performance metrics {ticker}...\")\n",
        "    print(f\"  MSE: {mse:.6f}\")\n",
        "    print(f\"  MAE: {mae:.6f}\")\n",
        "    print(f\"  R2: {r2:.6f}\")\n",
        "    # Time Series Cross-Validation\n",
        "    # Set up TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    cv_mse = []\n",
        "    cv_mae = []\n",
        "    cv_r2 = []\n",
        "    for train_idx, test_idx in tscv.split(X):\n",
        "      X_cv_train, X_cv_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "      y_cv_train, y_cv_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "      # Standardize\n",
        "      scaler = StandardScaler()\n",
        "      X_cv_train_scaled = scaler.fit_transform(X_cv_train)\n",
        "      X_cv_test_scaled = scaler.transform(X_cv_test)\n",
        "      model_cv = LinearRegression()\n",
        "      model_cv.fit(X_cv_train_scaled, y_cv_train)\n",
        "      y_cv_pred = model_cv.predict(X_cv_test_scaled)\n",
        "      # Calculate metrics\n",
        "      cv_mse.append(mean_squared_error(y_cv_test, y_cv_pred))\n",
        "      cv_mae.append(mean_absolute_error(y_cv_test, y_cv_pred))\n",
        "      cv_r2.append(r2_score(y_cv_test, y_cv_pred))\n",
        "    cv_results[model_name] = {\n",
        "        'mse': np.mean(cv_mse),\n",
        "        'mae': np.mean(cv_mae),\n",
        "        'r2': np.mean(cv_r2),\n",
        "        'mse_std': np.std(cv_mse),\n",
        "        'mae_std': np.std(cv_mae),\n",
        "        'r2_std': np.std(cv_r2)\n",
        "        }\n",
        "    print(f\"Cross-Validation {ticker}...\")\n",
        "    print(f\"  Avg MSE: {np.mean(cv_mse):.6f} (±{np.std(cv_mse):.6f})\")\n",
        "    print(f\"  Avg MAE: {np.mean(cv_mae):.6f} (±{np.std(cv_mae):.6f})\")\n",
        "    print(f\"  Avg R2: {np.mean(cv_r2):.6f} (±{np.std(cv_r2):.6f})\")\n",
        "    # Visualize the results\n",
        "    # Plot predictions vs actual\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    for i, (model_name, y_pred) in enumerate(predictions.items(), 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "        plt.plot(y_test.index, y_test.values, 'b-', label='Actual')\n",
        "        plt.plot(y_test.index, y_pred, 'r--', label=f'{model_name} Prediction')\n",
        "        plt.title(f'{ticker} - {model_name} Predictions')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Returns')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{ticker}_predictions.png')\n",
        "    # Plot feature importance for each model\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    for i, (model_name, importance_df) in enumerate(importances.items(), 1):\n",
        "        top_features = importance_df.head(10)\n",
        "        plt.subplot(2, 2, i)\n",
        "        plt.barh(top_features['Feature'], top_features['Importance'])\n",
        "        plt.title(f'{ticker} - {model_name} Feature Importance')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{ticker}_feature_importance.png')\n",
        "    # Store results for the ticker\n",
        "    results[ticker] = {\n",
        "        'data': data,\n",
        "        'eval_metrics': eval_metrics,\n",
        "        'cv_results': cv_results,\n",
        "        'importances': importances,\n",
        "        'predictions': predictions,\n",
        "        'y_test': y_test\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i1WzwUioe4W"
      },
      "source": [
        "* Support Vector Machines (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8NoUSUyyUfvW"
      },
      "outputs": [],
      "source": [
        "for ticker in tickers:\n",
        "    print(f\"Processing {ticker}...\")\n",
        "    #Data Preprocessing\n",
        "    data = fetch_and_process_data(ticker, market_index, period)\n",
        "    X = data[feature_cols]\n",
        "    y = data[target_col]\n",
        "    # Split data (keeping time order) - 80% training, 20% testing\n",
        "    train_size = int(len(X) * 0.8)\n",
        "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    # Model Implementation\n",
        "    # Support Vector Machines (SVM)\n",
        "    print(\"Training SVM...\")\n",
        "    # Hyperparameter tuning with TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    param_grid = {\n",
        "      'C': [0.01,0.1, 1, 10],  # Only 3 values\n",
        "      'gamma': [0.0001,0.001, 0.01, 0.1],\n",
        "      'kernel': ['rbf', 'poly','linear'],\n",
        "      'degree': [2, 3],  # Only used when kernel='poly'\n",
        "      'epsilon': [0.001,0.01, 0.1]\n",
        "    }\n",
        "    scoring = {\n",
        "      'neg_mean_absolute_error': 'neg_mean_absolute_error',\n",
        "      'r2': 'r2',\n",
        "      'neg_mean_squared_error': 'neg_mean_squared_error'\n",
        "    }\n",
        "    svr = SVR()\n",
        "    grid_search = GridSearchCV(svr, param_grid, cv=tscv,\n",
        "                         scoring=scoring,\n",
        "                         refit='neg_mean_absolute_error',\n",
        "                         n_jobs=-1)\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "    best_svr = grid_search.best_estimator_\n",
        "    svr_pred = best_svr.predict(X_test_scaled)\n",
        "    models['SVM'] = best_svr\n",
        "    predictions['SVM'] = svr_pred\n",
        "    print(f\"\\nBest SVM parameters for {ticker}: {grid_search.best_params_}\")\n",
        "    print(\"Best score (MSE):\", -grid_search.best_score_)\n",
        "    # Model Evaluation\n",
        "    # Create evaluation metrics dataframe\n",
        "    for model_name, y_pred in predictions.items():\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        eval_metrics = eval_metrics._append({\n",
        "            'Model': model_name,\n",
        "            'MSE': mse,\n",
        "            'MAE': mae,\n",
        "            'R2': r2\n",
        "        }, ignore_index=True)\n",
        "    # Print metrics\n",
        "    print(f\"Performance metrics {ticker}...\")\n",
        "    print(f\"  MSE: {mse:.6f}\")\n",
        "    print(f\"  MAE: {mae:.6f}\")\n",
        "    print(f\"  R2: {r2:.6f}\")\n",
        "    # Time Series Cross-Validation\n",
        "    # Set up TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    cv_mse = []\n",
        "    cv_mae = []\n",
        "    cv_r2 = []\n",
        "    for train_idx, test_idx in tscv.split(X):\n",
        "      X_cv_train, X_cv_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "      y_cv_train, y_cv_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "      # Standardize\n",
        "      scaler = StandardScaler()\n",
        "      X_cv_train_scaled = scaler.fit_transform(X_cv_train)\n",
        "      X_cv_test_scaled = scaler.transform(X_cv_test)\n",
        "      model_cv = SVR(**best_svr.get_params())\n",
        "      model_cv.fit(X_cv_train_scaled, y_cv_train)\n",
        "      y_cv_pred = model_cv.predict(X_cv_test_scaled)\n",
        "      # Calculate metrics\n",
        "      cv_mse.append(mean_squared_error(y_cv_test, y_cv_pred))\n",
        "      cv_mae.append(mean_absolute_error(y_cv_test, y_cv_pred))\n",
        "      cv_r2.append(r2_score(y_cv_test, y_cv_pred))\n",
        "    cv_results[model_name] = {\n",
        "        'MSE': np.mean(cv_mse),\n",
        "        'MAE': np.mean(cv_mae),\n",
        "        'R2': np.mean(cv_r2),\n",
        "        'mse_std': np.std(cv_mse),\n",
        "        'mae_std': np.std(cv_mae),\n",
        "        'r2_std': np.std(cv_r2)\n",
        "        }\n",
        "    print(f\"Cross-Validation {ticker}...\")\n",
        "    print(f\"  Avg MSE: {np.mean(cv_mse):.6f} (±{np.std(cv_mse):.6f})\")\n",
        "    print(f\"  Avg MAE: {np.mean(cv_mae):.6f} (±{np.std(cv_mae):.6f})\")\n",
        "    print(f\"  Avg R2: {np.mean(cv_r2):.6f} (±{np.std(cv_r2):.6f})\")\n",
        "    # Visualize the results\n",
        "    # Plot predictions vs actual\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    for i, (model_name, y_pred) in enumerate(predictions.items(), 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "        plt.plot(y_test.index, y_test.values, 'b-', label='Actual')\n",
        "        plt.plot(y_test.index, y_pred, 'r--', label=f'{model_name} Prediction')\n",
        "        plt.title(f'{ticker} - {model_name} Predictions')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Returns')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{ticker}_predictions.png')\n",
        "    # Store results for the ticker\n",
        "    results[ticker] = {\n",
        "        'data': data,\n",
        "        'eval_metrics': eval_metrics,\n",
        "        'cv_results': cv_results,\n",
        "        'predictions': predictions,\n",
        "        'y_test': y_test\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bApX3KAWsd_x"
      },
      "source": [
        "* Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jzeYkwipW5Qh"
      },
      "outputs": [],
      "source": [
        "for ticker in tickers:\n",
        "    print(f\"Processing {ticker}...\")\n",
        "    # Data Preprocessing\n",
        "    data = fetch_and_process_data(ticker, market_index, period)\n",
        "    X = data[feature_cols]\n",
        "    y = data[target_col]\n",
        "    # Split data (keeping time order) - 80% training, 20% testing\n",
        "    train_size = int(len(X) * 0.8)\n",
        "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    # Model Implementation\n",
        "    # Random Forest\n",
        "    print(\"Training Random Forest...\")\n",
        "    # Hyperparameter tuning\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2', None],\n",
        "        'oob_score': [True]\n",
        "    }\n",
        "    rf = RandomForestRegressor(random_state=42)\n",
        "    grid_search = GridSearchCV(rf, param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "    best_rf = grid_search.best_estimator_\n",
        "    rf_pred = best_rf.predict(X_test_scaled)\n",
        "    models['Random Forest'] = best_rf\n",
        "    predictions['Random Forest'] = rf_pred\n",
        "    # Feature importance for Random Forest\n",
        "    importances['Random Forest'] = pd.DataFrame({\n",
        "        'Feature': feature_cols,\n",
        "        'Importance': best_rf.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    print(f\"\\nBest Random Forest parameters for {ticker}: {grid_search.best_params_}\")\n",
        "    # Model Evaluation\n",
        "    # Create evaluation metrics dataframe\n",
        "    for model_name, y_pred in predictions.items():\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        eval_metrics = eval_metrics._append({\n",
        "            'Model': model_name,\n",
        "            'MSE': mse,\n",
        "            'MAE': mae,\n",
        "            'R2': r2\n",
        "        }, ignore_index=True)\n",
        "     # Print metrics\n",
        "    print(f\"Performance metrics {ticker}...\")\n",
        "    print(f\"  MSE: {mse:.6f}\")\n",
        "    print(f\"  MAE: {mae:.6f}\")\n",
        "    print(f\"  R2: {r2:.6f}\")\n",
        "    # Time Series Cross-Validation\n",
        "    # Set up TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    cv_mse = []\n",
        "    cv_mae = []\n",
        "    cv_r2 = []\n",
        "    for train_idx, test_idx in tscv.split(X):\n",
        "      X_cv_train, X_cv_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "      y_cv_train, y_cv_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "      # Standardize\n",
        "      scaler = StandardScaler()\n",
        "      X_cv_train_scaled = scaler.fit_transform(X_cv_train)\n",
        "      X_cv_test_scaled = scaler.transform(X_cv_test)\n",
        "      model_cv = RandomForestRegressor(**best_rf.get_params())\n",
        "      model_cv.fit(X_cv_train_scaled, y_cv_train)\n",
        "      y_cv_pred = model_cv.predict(X_cv_test_scaled)\n",
        "      # Calculate metrics\n",
        "      cv_mse.append(mean_squared_error(y_cv_test, y_cv_pred))\n",
        "      cv_mae.append(mean_absolute_error(y_cv_test, y_cv_pred))\n",
        "      cv_r2.append(r2_score(y_cv_test, y_cv_pred))\n",
        "    cv_results[model_name] = {\n",
        "        'MSE': np.mean(cv_mse),\n",
        "        'MAE': np.mean(cv_mae),\n",
        "        'R2': np.mean(cv_r2),\n",
        "        'mse_std': np.std(cv_mse),\n",
        "        'mae_std': np.std(cv_mae),\n",
        "        'r2_std': np.std(cv_r2)\n",
        "        }\n",
        "    print(f\"Cross-Validation {ticker}...\")\n",
        "    print(f\"  Avg MSE: {np.mean(cv_mse):.6f} (±{np.std(cv_mse):.6f})\")\n",
        "    print(f\"  Avg MAE: {np.mean(cv_mae):.6f} (±{np.std(cv_mae):.6f})\")\n",
        "    print(f\"  Avg R2: {np.mean(cv_r2):.6f} (±{np.std(cv_r2):.6f})\")\n",
        "    # Visualize the results\n",
        "    # Plot predictions vs actual\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    for i, (model_name, y_pred) in enumerate(predictions.items(), 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "        plt.plot(y_test.index, y_test.values, 'b-', label='Actual')\n",
        "        plt.plot(y_test.index, y_pred, 'r--', label=f'{model_name} Prediction')\n",
        "        plt.title(f'{ticker} - {model_name} Predictions')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Returns')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{ticker}_predictions.png')\n",
        "    # Plot feature importance for each model\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    for i, (model_name, importance_df) in enumerate(importances.items(), 1):\n",
        "        top_features = importance_df.head(10)\n",
        "        plt.subplot(2, 2, i)\n",
        "        plt.barh(top_features['Feature'], top_features['Importance'])\n",
        "        plt.title(f'{ticker} - {model_name} Feature Importance')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{ticker}_feature_importance.png')\n",
        "    # Store results for the ticker\n",
        "    results[ticker] = {\n",
        "        'data': data,\n",
        "        'eval_metrics': eval_metrics,\n",
        "        'cv_results': cv_results,\n",
        "        'importances': importances,\n",
        "        'predictions': predictions,\n",
        "        'y_test': y_test\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVnnEjmKs7JJ"
      },
      "source": [
        "* Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YJ4vocAYHZy"
      },
      "outputs": [],
      "source": [
        "nn_results = {}\n",
        "for ticker in tickers:\n",
        "  print(f\"Processing {ticker}...\")\n",
        "  # Data Preprocessing\n",
        "  data = fetch_and_process_data(ticker, market_index, period)\n",
        "  X = data[feature_cols]\n",
        "  y = data[target_col]\n",
        "  # Split data (keeping time order)\n",
        "  train_size = int(len(X) * 0.8)\n",
        "  X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "  y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "  # Standardize features\n",
        "  scaler = StandardScaler()\n",
        "  X_train_scaled = scaler.fit_transform(X_train)\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "  # Neural Network Model\n",
        "  print(\"Training Neural Network...\")\n",
        "  model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "  model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "  history = model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "  # Make predictions\n",
        "  nn_pred = model.predict(X_test_scaled).flatten()\n",
        "  models['Neural Networks'] = model\n",
        "  predictions['Neural Networks'] = nn_pred\n",
        "  # Calculate metrics\n",
        "  nn_mse = mean_squared_error(y_test, nn_pred)\n",
        "  nn_mae = mean_absolute_error(y_test, nn_pred)\n",
        "  nn_r2 = r2_score(y_test, nn_pred)\n",
        "  # Store results\n",
        "  nn_results[ticker] = {\n",
        "            'model': model,\n",
        "            'predictions': nn_pred,\n",
        "            'mse': nn_mse,\n",
        "            'mae': nn_mae,\n",
        "            'r2': nn_r2,\n",
        "            'history': history\n",
        "        }\n",
        "  # Print metrics\n",
        "  print(f\"{ticker}...\")\n",
        "  print(f\"  MSE: {nn_mse:.6f}\")\n",
        "  print(f\"  MAE: {nn_mae:.6f}\")\n",
        "  print(f\"  R2: {nn_r2:.6f}\")\n",
        "  # Time Series Cross-Validation\n",
        "  # Set up TimeSeriesSplit\n",
        "  tscv = TimeSeriesSplit(n_splits=5)\n",
        "  cv_mse = []\n",
        "  cv_mae = []\n",
        "  cv_r2 = []\n",
        "  for train_idx, test_idx in tscv.split(X):\n",
        "        X_cv_train, X_cv_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_cv_train, y_cv_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        # Standardize\n",
        "        scaler = StandardScaler()\n",
        "        X_cv_train_scaled = scaler.fit_transform(X_cv_train)\n",
        "        X_cv_test_scaled = scaler.transform(X_cv_test)\n",
        "        # Create and train NN\n",
        "        model_cv = Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(X_cv_train_scaled.shape[1],)),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model_cv.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        model_cv.fit(\n",
        "            X_cv_train_scaled, y_cv_train,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            validation_data=(X_cv_test_scaled, y_cv_test),\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "        y_cv_pred = model_cv.predict(X_cv_test_scaled).flatten()\n",
        "        # Calculate metrics\n",
        "        cv_mse.append(mean_squared_error(y_cv_test, y_cv_pred))\n",
        "        cv_mae.append(mean_absolute_error(y_cv_test, y_cv_pred))\n",
        "        cv_r2.append(r2_score(y_cv_test, y_cv_pred))\n",
        "  cv_results['Neural Networks'] = {\n",
        "        'MSE': np.mean(cv_mse),\n",
        "        'MAE': np.mean(cv_mae),\n",
        "        'R2': np.mean(cv_r2),\n",
        "        'mse_std': np.std(cv_mse),\n",
        "        'mae_std': np.std(cv_mae),\n",
        "        'r2_std': np.std(cv_r2)\n",
        "    }\n",
        "  print(f\"Cross-Validation {ticker}...\")\n",
        "  print(f\"  Avg MSE: {np.mean(cv_mse):.6f} (±{np.std(cv_mse):.6f})\")\n",
        "  print(f\"  Avg MAE: {np.mean(cv_mae):.6f} (±{np.std(cv_mae):.6f})\")\n",
        "  print(f\"  Avg R2: {np.mean(cv_r2):.6f} (±{np.std(cv_r2):.6f})\")\n",
        "    # Visualize the results\n",
        "    # Plot predictions vs actual\n",
        "  plt.figure(figsize=(14, 10))\n",
        "  for i, (model_name, y_pred) in enumerate(predictions.items(), 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "        plt.plot(y_test.index, y_test.values, 'b-', label='Actual')\n",
        "        plt.plot(y_test.index, y_pred, 'r--', label=f'{model_name} Prediction')\n",
        "        plt.title(f'{ticker} - {model_name} Predictions')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Returns')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(f'{ticker}_predictions.png')\n",
        "  # Store results for the ticker\n",
        "  results[ticker] = {\n",
        "        'data': data,\n",
        "        'eval_metrics': eval_metrics,\n",
        "        'cv_results': cv_results,\n",
        "        'predictions': predictions,\n",
        "        'y_test': y_test\n",
        "    }"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}